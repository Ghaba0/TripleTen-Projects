{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review**\n",
    "\n",
    "Hi, my name is Dmitry and I will be reviewing your project.\n",
    "  \n",
    "You can find my comments in colored markdown cells:\n",
    "  \n",
    "<div class=\"alert alert-success\">\n",
    "  If everything is done successfully.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-warning\">\n",
    "  If I have some (optional) suggestions, or questions to think about, or general comments.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-danger\">\n",
    "  If a section requires some corrections. Work can't be accepted with red comments.\n",
    "</div>\n",
    "  \n",
    "Please don't remove my comments, as it will make further review iterations much harder for me.\n",
    "  \n",
    "Feel free to reply to my comments or ask questions using the following template:\n",
    "  \n",
    "<div class=\"alert alert-info\">\n",
    "  For your comments and questions.\n",
    "</div>\n",
    "  \n",
    "First of all, thank you for turning in the project! You did an excellent job! The project is accepted. Keep up the good work on the next sprint!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "As a team member at Megaline, our goal is to enhance the subscriber experience by leveraging data-driven insights. We've identified a significant portion of our user base still on legacy plans, prompting us to develop a model to analyze subscriber behavior. Our aim is to recommend suitable plans from our newer offerings: Smart or Ultra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I was uncertain whether to retain the underperforming models or not, but ultimately, I decided to include them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Sure, why not? If you keep them, your research is reproducible!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>122.0</td>\n",
       "      <td>910.98</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35124.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>25.0</td>\n",
       "      <td>190.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3275.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>97.0</td>\n",
       "      <td>634.44</td>\n",
       "      <td>70.0</td>\n",
       "      <td>13974.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>64.0</td>\n",
       "      <td>462.32</td>\n",
       "      <td>90.0</td>\n",
       "      <td>31239.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>80.0</td>\n",
       "      <td>566.09</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29480.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3214 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      calls  minutes  messages   mb_used  is_ultra\n",
       "0      40.0   311.90      83.0  19915.42         0\n",
       "1      85.0   516.75      56.0  22696.96         0\n",
       "2      77.0   467.66      86.0  21060.45         0\n",
       "3     106.0   745.53      81.0   8437.39         1\n",
       "4      66.0   418.74       1.0  14502.75         0\n",
       "...     ...      ...       ...       ...       ...\n",
       "3209  122.0   910.98      20.0  35124.90         1\n",
       "3210   25.0   190.36       0.0   3275.61         0\n",
       "3211   97.0   634.44      70.0  13974.06         0\n",
       "3212   64.0   462.32      90.0  31239.78         0\n",
       "3213   80.0   566.09       6.0  29480.52         1\n",
       "\n",
       "[3214 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_behavior = pd.read_csv('/datasets/users_behavior.csv')\n",
    "display(user_behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "When loading the data, it's a good idea to at least take a look at it using `pd.DataFrame.info()` or `pd.DataFrame.head()` to make sure everything loaded correctly :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize temp train df and test df\n",
    "# I split 20% because it is a large dataset and I'm splitting it twice which would only leave 50% for training\n",
    "ub_train_temp, ub_test = train_test_split(\n",
    "    user_behavior, \n",
    "    test_size=.2, # Split 20% of data to make test set\n",
    "    random_state=12345)\n",
    "\n",
    "# initialize valid df and actual train df\n",
    "ub_train, ub_valid = train_test_split(\n",
    "    ub_train_temp, \n",
    "    test_size=0.2,  # Split another 20% of data to make validation set\n",
    "    random_state=54321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The data split is reasonable!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "features = user_behavior.drop(['is_ultra'], axis=1)\n",
    "target = user_behavior['is_ultra']\n",
    "\n",
    "features_train = ub_train.drop(['is_ultra'], axis=1)\n",
    "target_train = ub_train['is_ultra']\n",
    "\n",
    "features_test = ub_test.drop(['is_ultra'], axis=1)\n",
    "target_test = ub_test['is_ultra']\n",
    "\n",
    "features_valid = ub_valid.drop(['is_ultra'], axis=1)\n",
    "target_valid = ub_valid['is_ultra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Good!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=12345, solver='liblinear')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# initialize model\n",
    "logReg = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "logReg.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7359223300970874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on valid df using Logistic Regression\n",
    "logReg_predictions_valid = logReg.predict(features_valid)\n",
    "\n",
    "# Get logReg accuracy\n",
    "logReg_accuracy = accuracy_score(target_valid, logReg_predictions_valid)\n",
    "\n",
    "print(\"Accuracy:\", logReg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=12345)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=12345, max_depth=3)\n",
    "tree.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7786407766990291\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on valid df using Decision Tree Classifier\n",
    "tree_predictions_valid = tree.predict(features_valid)\n",
    "\n",
    "# Get tree accuracy\n",
    "tree_accuracy = accuracy_score(target_valid, tree_predictions_valid)\n",
    "\n",
    "print(\"Accuracy:\", tree_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy to Depth Chart for Decision Tree\n",
    "\n",
    "max_depth=6: 0.7650485436893204\n",
    "\n",
    "max_depth=5: 0.7728155339805826\n",
    "\n",
    "max_depth=4: 0.7747572815533981\n",
    "\n",
    "**max_depth=3: 0.7786407766990291**\n",
    "\n",
    "max_depth=2: 0.7533980582524272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model on the validation set (n_estimators = 10): 0.7980582524271844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 11): # Limited to 1 - 10 as to not overfit the model\n",
    "    forest = RandomForestClassifier(random_state=54321, n_estimators=est) # set number of trees\n",
    "    forest.fit(features_train, target_train) # train model on training set\n",
    "    score = forest.score(features_valid, target_valid) # calculate accuracy score on validation set\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_est = est\n",
    "\n",
    "print(\"Accuracy of the best model on the validation set (n_estimators = {}): {}\".format(best_est, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Great, you tried a couple of different models and did some hyperparameter tuning using the validation set\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier has the highest accuracy out of the three I've tested but the slowest speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7947122861586314\n"
     ]
    }
   ],
   "source": [
    "# Testing forest df quality with test set\n",
    "forest_predictions_test = forest.predict(features_test)\n",
    "\n",
    "fpt_accuracy = accuracy_score(target_test, forest_predictions_test)\n",
    "\n",
    "print(fpt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "The final model was evaluated on the test set\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar accuracy to the training set 👍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "You mean the validation set, right? :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 175\n",
      "Actual Value: 0\n",
      "Predicted Value: 0\n",
      "\n",
      "Index: 609\n",
      "Actual Value: 0\n",
      "Predicted Value: 0\n",
      "\n",
      "Index: 218\n",
      "Actual Value: 0\n",
      "Predicted Value: 0\n",
      "\n",
      "Index: 538\n",
      "Actual Value: 1\n",
      "Predicted Value: 0\n",
      "\n",
      "Index: 147\n",
      "Actual Value: 0\n",
      "Predicted Value: 0\n",
      "\n",
      "Index: 320\n",
      "Actual Value: 1\n",
      "Predicted Value: 0\n",
      "\n",
      "Index: 475\n",
      "Actual Value: 0\n",
      "Predicted Value: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "import random\n",
    "\n",
    "# Choose a few random indices from the test set\n",
    "random_indices = random.sample(range(len(features_test)), 7)\n",
    "\n",
    "# Print actual and predicted values for these indices\n",
    "for idx in random_indices:\n",
    "    actual_value = target_test.iloc[idx]\n",
    "    predicted_value = forest.predict(features_test.iloc[[idx]])[0]  # Assuming a classification model\n",
    "    \n",
    "    print(\"Index:\", idx)\n",
    "    print(\"Actual Value:\", actual_value)\n",
    "    print(\"Predicted Value:\", predicted_value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "A better way to sanity check your model is to compare it to some kind of a baseline. For example, here we can take a constant model always predicting the majority class. Its accuracy is equal to the share of the majority class (about 70% in this case). Our model is better than that, so it probably learned something useful :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Reviewed the original dataset 'users_behavior.csv' to identify features, target, and determine whether the problem was classification or regression.\n",
    "- Split the dataset into training and testing sets using a ratio of 60:20:20 to ensure an adequate portion of the training data.\n",
    "- Evaluated the performance of different models, including LogisticRegression, DecisionTreeClassifier, and RandomForestClassifier.\n",
    "- Observed that RandomForestClassifier achieved the highest accuracy among the tested models.\n",
    "- Identified LogisticRegression as the least accurate model out of the three tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\n",
    "Conclusions make sense\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
